{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16320875724"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlx.core.metal as metal\n",
    "# set memory limit to 3GB\n",
    "metal.set_memory_limit(3*1024*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load pre-train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gemma import InfiniModel\n",
    "from mlx_lm.tokenizer_utils import load_tokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./gemma-1.1-2b-it-4bit-128gs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:gate weights not found in the model weights. Initializing with 0.\n"
     ]
    }
   ],
   "source": [
    "model = InfiniModel.from_pretrain(MODEL_PATH)\n",
    "tokenizer = load_tokenizer(Path(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"Stop here\")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Salesforce/wikitext\", 'wikitext-2-raw-v1', cache_dir=\"./datasets/wikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = tokenizer.encode(text['text'])\n",
    "    tokens.append(tokenizer.eos_token_id)\n",
    "    return {'text': tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"Stop here\")\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "from models.base import MemoryCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model: InfiniModel, inputs, cache):\n",
    "    output = inputs[:, 1:]\n",
    "    inputs = inputs[:, :-1]\n",
    "    B, L = inputs.shape\n",
    "    pred = [None for _ in range(L)]\n",
    "    def add_additive(x: mx.array):\n",
    "        # x: (B, V)\n",
    "        V = model.args.vocab_size\n",
    "        ADDITIVE = 1e-5\n",
    "        x_sum = x.sum()\n",
    "        x = (x + ADDITIVE)/(x_sum + ADDITIVE * V)\n",
    "        return x\n",
    "\n",
    "    for i in range(0, L):\n",
    "        input = mx.array(inputs[:, i])[None].reshape(B, 1)\n",
    "        # print(\"input: \", input.shape)\n",
    "        pred[i] = model(input, cache=cache, is_training=False)[:, -1, :]\n",
    "        # pred[i] = add_additive(pred[i])\n",
    "        # print(\"pred: \", pred[i].shape)\n",
    "    preds = mx.concatenate([mx.expand_dims(i, axis=1) for i in pred], axis=1)\n",
    "    preds.reshape(B, L, -1)\n",
    "    print(L, preds.shape)\n",
    "    return mx.mean(nn.losses.cross_entropy(preds, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "mx.eval(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.utils as mxu\n",
    "\n",
    "def clip_nan_grads(grads):\n",
    "    def deal_nan(x: mx.array):\n",
    "        shape = x.shape\n",
    "\n",
    "        # flatten the array (mx.flatten is not working here)\n",
    "        for _ in range(len(shape)-1):\n",
    "            x = mx.concatenate([mx.array(item) for item in x.tolist()]) if x.shape[0] != 1 else x[0]\n",
    "\n",
    "        for i, has_nan in enumerate(mx.isnan(x).tolist()):\n",
    "            if has_nan:\n",
    "                x[i] = 0\n",
    "        x = x.reshape(shape)\n",
    "        return x\n",
    "    \n",
    "    grads.update(mxu.tree_map(deal_nan, grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6652, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6652, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6651, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6651, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.665, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.665, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6649, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6649, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6649, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6648, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6648, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6647, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6647, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6646, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6646, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6645, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6645, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6644, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6644, dtype=float32)\n",
      "9 (1, 9, 256000)\n",
      "Loss: array(25.6643, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# use single sentence for here\n",
    "data = dataset['train'][1]\n",
    "if len(data['text']) > 20:\n",
    "    print(\"Too long\", len(data['text']))\n",
    "    data['text'] = data['text'][:20]\n",
    "for _ in range(20):\n",
    "    L = 1\n",
    "    cache = [(mx.zeros((1 ,L, model.head_dim, model.head_dim)), mx.ones((1, L, model.head_dim, 1))) for _ in range(len(model.layers))]\n",
    "\n",
    "    inputs = mx.array(data['text'])[None]\n",
    "    loss, grads = loss_and_grad_fn(model, inputs, cache)\n",
    "    clip_nan_grads(grads)\n",
    "    optimizer.update(model, grads)\n",
    "    mx.eval(model.parameters(), optimizer.state)\n",
    "\n",
    "    print(\"Loss:\", loss)\n",
    "# for i in range(18):\n",
    "#     print(f\"Layer {i} Grads\", grads['model']['layers'][i]['self_attn']['gate'])\n",
    "# print(\"Grads\", grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'embed_tokens': {},\n",
       "  'layers': [{'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[-0.000513476],\n",
       "              [-0.000228157],\n",
       "              [0.00050825],\n",
       "              ...,\n",
       "              [0.0011174],\n",
       "              [-0.000636316],\n",
       "              [-0.000164032]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([-1, 3.10938, 0.660156, ..., 2.4375, 2.65625, 2.78125], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([1.44531, 1.91406, 1.72656, ..., 1.63281, 2.01562, 1.48438], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[-0.000121464],\n",
       "              [-0.000143831],\n",
       "              [-3.81013e-05],\n",
       "              ...,\n",
       "              [-1.61331e-05],\n",
       "              [-4.27555e-05],\n",
       "              [-3.64272e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.125, 0.808594, 1.11719, ..., 1.07812, 0.976562, 0.804688], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([1.72656, 1.47656, 1.72656, ..., 1.40625, 1.74219, 1.20312], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[-1.43428e-05],\n",
       "              [-5.91484e-05],\n",
       "              [4.66097e-05],\n",
       "              ...,\n",
       "              [-0.000111518],\n",
       "              [3.06982e-05],\n",
       "              [-4.42262e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.39062, 1.14844, 1.41406, ..., 1.30469, 1.44531, 1], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.59375, 1.79688, 2.57812, ..., 1.95312, 2.70312, 1.48438], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[-8.1201e-05],\n",
       "              [-2.02041e-05],\n",
       "              [-9.65589e-05],\n",
       "              ...,\n",
       "              [-9.06881e-05],\n",
       "              [0.000135526],\n",
       "              [1.74207e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.60156, 0.796875, 1.03906, ..., 0.601562, 1, 0.408203], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.89062, 1.64062, 2.67188, ..., 1.83594, 2.79688, 1.45312], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[-0.000211242],\n",
       "              [-6.86099e-05],\n",
       "              [2.58917e-05],\n",
       "              ...,\n",
       "              [-0.000124407],\n",
       "              [-7.08248e-05],\n",
       "              [2.49563e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.17969, 0.910156, 1.01562, ..., 0.65625, 1.45312, 0.357422], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.8125, 1.48438, 2.67188, ..., 1.75, 3.04688, 1.25781], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[0.000212941],\n",
       "              [-0.000123928],\n",
       "              [-6.65099e-05],\n",
       "              ...,\n",
       "              [-4.03721e-06],\n",
       "              [0.000119321],\n",
       "              [-1.18505e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.61719, 0.925781, 1.45312, ..., 0.707031, 1.75, 0.378906], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([3.03125, 1.35938, 3.09375, ..., 1.82812, 3.48438, 1.34375], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[-6.8083e-05],\n",
       "              [-5.81675e-06],\n",
       "              [-2.79881e-05],\n",
       "              ...,\n",
       "              [4.14247e-05],\n",
       "              [2.06823e-05],\n",
       "              [-9.83681e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.99219, 0.996094, 2.21875, ..., 0.90625, 3.1875, 0.306641], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([3, 1.26562, 3.0625, ..., 1.78125, 3.96875, 1.42188], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[4.92825e-05],\n",
       "              [-7.52584e-06],\n",
       "              [-1.4325e-05],\n",
       "              ...,\n",
       "              [8.70231e-06],\n",
       "              [2.69928e-05],\n",
       "              [-7.37966e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.39062, 0.863281, 1.41406, ..., 1.01562, 1.92969, 0.515625], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.85938, 1.13281, 2.9375, ..., 1.60938, 3.625, 1.21094], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[1.25195e-05],\n",
       "              [-9.9967e-05],\n",
       "              [-0.000200093],\n",
       "              ...,\n",
       "              [1.76247e-05],\n",
       "              [0.000182288],\n",
       "              [-2.39978e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.50781, 0.789062, 1.75781, ..., 1.01562, 2.71875, 0.566406], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.78125, 1.02344, 2.8125, ..., 1.42969, 3.51562, 1.125], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[1.39442e-05],\n",
       "              [1.93247e-06],\n",
       "              [1.811e-05],\n",
       "              ...,\n",
       "              [-2.13634e-05],\n",
       "              [-2.26712e-06],\n",
       "              [1.95277e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.26562, 0.589844, 1.44531, ..., 0.455078, 2.48438, 0.00982666], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.5625, 0.945312, 2.64062, ..., 1.25, 3.375, 0.871094], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[-6.61573e-05],\n",
       "              [-0.000133346],\n",
       "              [-1.05645e-05],\n",
       "              ...,\n",
       "              [-2.00926e-05],\n",
       "              [-2.84511e-05],\n",
       "              [5.85817e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.625, 0.882812, 1.85156, ..., 0.824219, 3.14062, 0.242188], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.60938, 1.11719, 2.64062, ..., 1.45312, 3.17188, 1.14062], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[-5.24192e-05],\n",
       "              [1.41548e-05],\n",
       "              [0.000208206],\n",
       "              ...,\n",
       "              [0.000177713],\n",
       "              [-6.26608e-05],\n",
       "              [5.62122e-06]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.54688, 1.32812, 1.90625, ..., 1.10156, 2.1875, 0.640625], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.35938, 1.1875, 2.42188, ..., 1.47656, 2.79688, 1.22656], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[-4.76402e-05],\n",
       "              [2.70817e-05],\n",
       "              [4.48034e-05],\n",
       "              ...,\n",
       "              [-0.000121707],\n",
       "              [-1.59347e-05],\n",
       "              [2.77046e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.44531, 1.10938, 1.80469, ..., 1.42188, 2.125, 1.10156], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.4375, 1.42188, 2.42188, ..., 1.83594, 2.60938, 1.63281], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[2.27239e-05],\n",
       "              [3.15849e-05],\n",
       "              [-5.09192e-06],\n",
       "              ...,\n",
       "              [1.17385e-05],\n",
       "              [7.42747e-05],\n",
       "              [2.57317e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.125, 1.35156, 1.15625, ..., 1.17188, 1.39844, 0.773438], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.40625, 1.67969, 2.45312, ..., 2.01562, 2.73438, 1.92188], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[0.000199939],\n",
       "              [0.000138958],\n",
       "              [-0.0002525],\n",
       "              ...,\n",
       "              [0.00014311],\n",
       "              [7.21558e-05],\n",
       "              [-6.16943e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([0.648438, 1.42188, 1.04688, ..., 0.710938, 1.03125, 0.726562], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.65625, 1.92969, 2.59375, ..., 2.45312, 2.76562, 2.39062], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[0.000141077],\n",
       "              [7.65682e-05],\n",
       "              [3.8365e-05],\n",
       "              ...,\n",
       "              [0.000213374],\n",
       "              [0.000112073],\n",
       "              [8.22589e-05]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([0.734375, 1.6875, 1.00781, ..., 0.808594, 0.992188, 1.5], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.70312, 2.28125, 2.71875, ..., 2.60938, 2.85938, 2.59375], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[9.65362e-05],\n",
       "              [-0.000104588],\n",
       "              [4.05469e-05],\n",
       "              ...,\n",
       "              [-1.70211e-05],\n",
       "              [7.39753e-05],\n",
       "              [-5.64498e-06]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([0.835938, 1.52344, 1.23438, ..., 0.878906, 1.375, 0.96875], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.64062, 2.46875, 2.8125, ..., 2.67188, 2.85938, 2.70312], dtype=float16)}},\n",
       "   {'self_attn': {'q_proj': {},\n",
       "     'k_proj': {},\n",
       "     'v_proj': {},\n",
       "     'o_proj': {},\n",
       "     'rope': {},\n",
       "     'gate': array([[[[3.31946e-05],\n",
       "              [-1.16505e-06],\n",
       "              [-2.52001e-05],\n",
       "              ...,\n",
       "              [-6.20418e-05],\n",
       "              [2.61127e-05],\n",
       "              [-0.000202475]]]], dtype=float32)},\n",
       "    'mlp': {'gate_proj': {}, 'down_proj': {}, 'up_proj': {}},\n",
       "    'input_layernorm': {'weight': array([1.34375, 2.10938, 1.44531, ..., 1.67188, 1.59375, 1.64062], dtype=float16)},\n",
       "    'post_attention_layernorm': {'weight': array([2.26562, 2.8125, 2.95312, ..., 2.71875, 2.64062, 2.84375], dtype=float16)}}],\n",
       "  'norm': {'weight': array([0.157227, 0.617188, 0.460938, ..., 0.609375, 0.378906, 0.644531], dtype=float16)}}}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 236280]\n",
      "(1, 1, 256000)\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "# raise Exception(\"Stop here\")\n",
    "promptToken = tokenizer.encode('hi')\n",
    "print(promptToken)\n",
    "cache = [(mx.zeros((1 ,1, model.head_dim, model.head_dim)), mx.ones((1, 1, model.head_dim, 1))) for _ in range(len(model.layers))]\n",
    "pred = model(mx.array(promptToken[1])[None][None], cache, False)\n",
    "print(pred.shape)\n",
    "detokenizer = tokenizer.detokenizer\n",
    "detokenizer.reset()\n",
    "detokenizer.add_token(pred.argmax().item())\n",
    "detokenizer.finalize()\n",
    "print(detokenizer.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Stop here"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Stop here\")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loss_fn() missing 1 required positional argument: 'cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m mx\u001b[38;5;241m.\u001b[39marray(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][i])[\u001b[38;5;28;01mNone\u001b[39;00m][\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m mx\u001b[38;5;241m.\u001b[39marray(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m])[\u001b[38;5;28;01mNone\u001b[39;00m][\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m----> 9\u001b[0m loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mloss_and_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mupdate(model, grads)\n\u001b[1;32m     11\u001b[0m mx\u001b[38;5;241m.\u001b[39meval(model\u001b[38;5;241m.\u001b[39mparameters(), optimizer\u001b[38;5;241m.\u001b[39mstate)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx-env/lib/python3.12/site-packages/mlx/nn/utils.py:34\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.wrapped_value_grad_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_value_grad_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 34\u001b[0m     value, grad \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value, grad\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx-env/lib/python3.12/site-packages/mlx/nn/utils.py:28\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.inner_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_fn\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m.\u001b[39mupdate(params)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: loss_fn() missing 1 required positional argument: 'cache'"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for data in dataset['train']:\n",
    "    if len(data['text']) < 3:\n",
    "        continue\n",
    "    L = len(data['text']) - 2\n",
    "    for i in range(0, L):\n",
    "        inputs = mx.array(data['text'][i])[None][None]\n",
    "        output = mx.array(data['text'][i+2])[None][None]\n",
    "        loss, grads = loss_and_grad_fn(model, inputs, output)\n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "\n",
    "    print(\"Loss:\", loss)\n",
    "    count += 1\n",
    "    # inputs = mx.array(data['text'][:-2])[None]\n",
    "    # output = mx.array(data['text'][2:])[None]\n",
    "    # loss, grads = loss_and_grad_fn(model, inputs, output)\n",
    "    # optimizer.update(model, grads)\n",
    "    # mx.eval(model.parameters(), optimizer.state)\n",
    "    # acc = eval_fn(model, mx.array(data['text'][:-2])[None], mx.array(data['text'][2:])[None])\n",
    "    # print(\"Accuracy:\", acc, \"Loss:\", loss)\n",
    "    # print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTokens = tokenizer.encode(\"hi\")\n",
    "promptTokens.append(tokenizer.eos_token_id)\n",
    "logits = model(mx.array(promptTokens)[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token:  fta\n"
     ]
    }
   ],
   "source": [
    "detokenizer = tokenizer.detokenizer\n",
    "detokenizer.reset()\n",
    "detokenizer.add_token(logits.argmax().item())\n",
    "detokenizer.finalize()\n",
    "print(\"Next token:\", detokenizer.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
